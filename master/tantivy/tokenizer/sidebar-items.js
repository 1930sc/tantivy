initSidebarItems({"fn":[["assert_token","This is a function that can be used in tests and doc tests to assert a token's correctness. TODO: can this be wrapped in #[cfg(test)] so as not to be in the public api?"]],"struct":[["AlphaNumOnlyFilter","`TokenFilter` that removes all tokens that contain non ascii alphanumeric characters."],["FacetTokenizer","The `FacetTokenizer` process a `Facet` binary representation and emits a token for all of its parent."],["JapaneseTokenizer","Simple japanese tokenizer based on the `tinysegmenter` crate."],["LowerCaser","Token filter that lowercase terms."],["NgramTokenizer","Tokenize the text by splitting words into n-grams of the given size(s)"],["RawTokenizer","For each value of the field, emit a single unprocessed token."],["RemoveLongFilter","`RemoveLongFilter` removes tokens that are longer than a given number of bytes (in UTF-8 representation)."],["SimpleTokenizer","Tokenize the text by splitting on whitespaces and punctuation."],["Stemmer","`Stemmer` token filter. Currently only English is supported. Tokens are expected to be lowercased beforehands."],["StopWordFilter","`TokenFilter` that removes stop words from a token stream"],["Token","Token"],["TokenizerManager","The tokenizer manager serves as a store for all of the pre-configured tokenizer pipelines."]],"trait":[["BoxedTokenizer","A boxed tokenizer"],["TokenFilter","Trait for the pluggable components of `Tokenizer`s."],["TokenStream","`TokenStream` is the result of the tokenization."],["Tokenizer","`Tokenizer` are in charge of splitting text into a stream of token before indexing."]]});